# @package _global_

# tensorboard --host localhost --port 8080 --logdir=./outputs

run_type: "train"
debug_lvl: 0  # 0 disables debugging and verbosity completely, >1 activates additional debugging functionality
global_seed: 1234
seed: 1234
cuda: True

eval_only: False

train_cfg:
  get_val_set: True

val_size: 1000 # 1000           # Number of instances used for reporting validation performance
val_batch_size: 100 # 1000           # Number of instances used for reporting validation performance
val_m: 1                       # number of data augments (<=8)
val_dataset: # None            # Dataset file to use for validation
batch_size: 512 # 64    # original 600
epoch_end: 200 # 00   # help='maximum training epoch'
T_train: 200 # 200   # help='number of itrations for training ' -- > same as above? - 250 for CVRP / 200 for TSP
T_max: 1000 # ${T_max} # 5000    # number of steps for inference --> 1000 in training
stall: 0  # actually same as stall_limit --> but for some reason renamed
stall_limit: 0
epoch_size: 10240   # help='number of instances per epoch during training'  # 10240 for TSP
checkpoint_epochs: 50
record: # --> if no value set - will not record training process

# model env cfg
env_cfg:
  #step_method: '2_opt'    # ['2_opt','swap','insert'] ---> DACT
  init_val_met: random  # ['random','greedy','seq']
  # perturb_eps: 250    # eval ---> DACT
  # perturb_eps: 1e10   # train ---> DACT
  dummy_rate: ${dummy_rate}  # # 0.5, 0.4, 0.2 for CVRP20, 50, 100, respectively


# PPO Agent settings
K_epochs: 4     # help='mini PPO epoch '
# best_cl: False    # help='use best solution found in CL as initial solution for training '
# Xi_CL: 0.25     # help='hyperparameter of CL '
lr_model: 8e-5   # help="learning rate for the actor network")
lr_critic: 2e-5   # help="learning rate for the critic network")
lr_decay: 0.985   # help='learning rate decay per epoch '
# warm_up: ${warm_up}  # 2 # rho in the paper --> depends on size
max_grad_norm: 0.05   # help='maximum L2 norm for gradient clipping '
eps_clip: 0.1   # help='PPO clip ratio '
n_step: 5     # help='n_step for return estimation ' --> 4 for TSP, 5 for CVRP

# resume and load models
load_path:                        # path to model params and optimizer
resume:                           # resume from prev. checkpoint file
epoch_start: 0                    # start at epoch

test_cfg:
  data_file_path: # None         # data/test_data/cvrp/uniform/cvrp20_test_seed1234.pkl
  checkpoint_load_path: #None
  saved_res_dir: #

# global paths and logging
log_lvl: INFO
tb_logging: False
no_tb: True
tb_log_path: 'logs/tb/'
log_path: 'logs/'
checkpoint_save_path: 'checkpoints/'
save_dir: 'checkpoints/'
no_saving: False # if True - disabled checkpointing (store-true action! - False will still cound as action)
