# @package _global_
# policy config

policy: "NeuOpt"



# model params
v_range: 6.0    # help='to control the entropy'
critic_head_num: 4   # help='head number of NeuOpt critic'
actor_head_num: 4   # help='head number of NeuOpt actor'
# critic_head_num: 6  # help='head number of critic encoder'
embedding_dim: 128  # help='dimension of input embeddings (NEF & PFE)'
hidden_dim: 128  # help='dimension of hidden layers in Enc/Dec'
n_encode_layers: 3  # help='number of stacked layers in the encoder'
normalization: 'layer'  # help="normalization type, #'layer' (default) or 'batch'"
gamma: 0.999    # help='reward discount factor for future rewards '
# T_max: 1000 # 5000    # number of steps for inference --> in run.yaml
num_augments: 1  # number of data augments (<=8) # ? can't find that parameter in options.py (from original code)
K_epochs: 3     # help='mini PPO epoch '

# agent params
# RL_agent: 'ppo' # help='RL Training algorithm '
wo_bonus: false #  to remove reward shaping term
wo_regular: false   # when training TSP - should be True --> see runner init PPO
wo_RNN: false # to remove RNN
# wo_feature1: ${wo_feature1} # remove VI features
# wo_feature3: ${wo_feature3} # remove ES features
wo_MDP: true # always True (disabled function)


# disable distributed run
no_DDP: false